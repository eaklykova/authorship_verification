{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Verification Using Common N-Gram Profiles of Text Documents #\n",
    "### Создатели: Алла Горбунова, Лика Джиоева, Евгения Егорова, Елизавета Клыкова и Яна Шишкина \n",
    "#### C опорой на (Magdalena Jankowska, Evangelos Milios & Vlado Kešelj, 2014) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В папке с тетрадкой должны быть следующие папки:\n",
    "* answers - содержит файлы truth.txt и truth-test.txt\n",
    "* texts_train - содержит папки с данными для обучения\n",
    "* texts_test - содержит папки с тестовыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(directory):  # получает тексты и информацию о них\n",
    "    dirs = [directory + d + '/' for d in os.listdir(directory)]\n",
    "    texts_info = []\n",
    "    for d in dirs:\n",
    "        files = [d + f for f in os.listdir(d)]\n",
    "        lang = re.search('[A-Z]+', d).group().lower()\n",
    "        author = re.search('[0-9]+', d.lower()).group()\n",
    "        for f in files:\n",
    "            info = []\n",
    "            info.append(author)\n",
    "            info.append(lang)\n",
    "            with open(f, encoding='utf-8-sig') as f1:\n",
    "                text = f1.read()\n",
    "                info.append(text)\n",
    "                info.append(len(text.split()))\n",
    "            texts_info.append(info)\n",
    "    return texts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(texts):  # создает датафрейм\n",
    "    columns = ['author', 'lang', 'text', 'length']\n",
    "    df = pd.DataFrame(texts, columns=columns)\n",
    "    df.index.name = 'id'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):  # возвращает список слов без пунктуации\n",
    "    punct = list(re.sub(\"[-']\", '', string.punctuation))\n",
    "    other_punct = ['``', '\\\"\\\"', '...', '--', '–', '—',\n",
    "                   '«', '»', '“', '”', '’', '***', '…', '•']\n",
    "    all_punct = punct + other_punct\n",
    "    rx = '[' + re.escape(''.join(all_punct)) + ']'\n",
    "    word_list = word_tokenize((re.sub(rx, ' ', text.lower())))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_freq(ngrams):  # возвращает частотности n-грамм текста\n",
    "    for key, num in ngrams.items():\n",
    "        freq = num / len(ngrams)\n",
    "        ngrams[key] = freq\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_ngrams(words):  # возвращает 1-2-3-граммы слов\n",
    "    word_unigrams = dict(Counter(words).most_common())\n",
    "    freq_unigrams = add_freq(word_unigrams)\n",
    "\n",
    "    bigrams = [' '.join(b) for b in list(nltk.bigrams(words))]\n",
    "    freq_bigrams = add_freq(dict(Counter(bigrams).most_common()))\n",
    "\n",
    "    trigrams = [' '.join(b) for b in list(nltk.trigrams(words))]\n",
    "    freq_trigrams = add_freq(dict(Counter(trigrams).most_common()))\n",
    "\n",
    "    return freq_unigrams, freq_bigrams, freq_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_char_ngrams(text, n):  # возвращает 3-7-граммы символов\n",
    "    my_ngrams = [''.join(ng) for ng in list(ngrams(text, n))]\n",
    "    freq_ngrams = add_freq(dict(Counter(my_ngrams).most_common()))\n",
    "    return freq_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Впомогательные функции\n",
    "* Обрезка по минимальной длине профиля\n",
    "* Заполнение нулями отсутствующих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_profile(d, all_docs):  # обрезает все профили по самому короткому\n",
    "    lens = [len(ngrams) for ngrams in all_docs]\n",
    "    num = min(lens)\n",
    "    d_cut = dict(Counter(d).most_common(num))\n",
    "    return d_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros(d, keys):  # заменяет пустые частотности на 0.0\n",
    "    dif_keys = keys - set(d.keys())\n",
    "    for d_key in dif_keys:\n",
    "        d[d_key] = 0.0\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(d1_, d2_):  # вычисляет difference\n",
    "    d1 = d1_.copy()\n",
    "    d2 = d2_.copy()\n",
    "    keys = set(d1.keys()) | set(d2.keys())\n",
    "    add_zeros(d1, keys)\n",
    "    add_zeros(d2, keys)\n",
    "    summ = 0\n",
    "    for key in keys:\n",
    "        summ = summ + (2 * (d1[key] - d2[key]) / (d1[key] + d2[key])) ** 2\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dif(known_docs_cut):  # вычисляет Dmax\n",
    "    max_list = []\n",
    "    for d_target in known_docs_cut:\n",
    "        difs = [difference(d_target, d) for d in known_docs_cut]\n",
    "        max_list.append(max(difs))\n",
    "    return max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_ratio(known_docs, unknown_doc):  # средний коэф. различия\n",
    "    known_docs_cut = [cut_profile(d, known_docs) for d in known_docs]\n",
    "    max_list = find_max_dif(known_docs_cut)\n",
    "    dif_unknown = [difference(unknown_doc, d) for d in known_docs_cut]\n",
    "\n",
    "    ratios = []\n",
    "    for i in range(len(max_list)):\n",
    "        r = dif_unknown[i]/max_list[i]\n",
    "        ratios.append(r)\n",
    "\n",
    "    return np.mean(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(known_profile, unknown_profile, theta):  # выдает ответ\n",
    "    mean_r = find_mean_ratio(known_profile, unknown_profile)\n",
    "    if mean_r <= theta:\n",
    "        return 'Y', mean_r\n",
    "    else:\n",
    "        return 'N', mean_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_authorship(batch, theta, n_char=None, n_word=1):\n",
    "    known_texts = batch.iloc[:-1].text\n",
    "    if len(known_texts) == 1:\n",
    "        text1 = known_texts.iloc[0][:len(known_texts.iloc[0])//2]\n",
    "        text2 = known_texts.iloc[0][len(known_texts.iloc[0])//2:]\n",
    "        known_texts = [text1, text2]\n",
    "    known_words = [preprocess(text) for text in known_texts]\n",
    "\n",
    "    d_char_ngrams = []\n",
    "    if n_char is not None:\n",
    "        for text in known_texts:\n",
    "            d_char_ngrams.append(make_char_ngrams(text, n_char))\n",
    "\n",
    "    d_unigrams, d_bigrams, d_trigrams = [], [], []\n",
    "    for i, d in enumerate(known_words):\n",
    "        unigrams, bigrams, trigrams = make_word_ngrams(d)\n",
    "        d_unigrams.append(unigrams)\n",
    "        d_bigrams.append(bigrams)\n",
    "        d_trigrams.append(trigrams)\n",
    "    d_word_ngrams = [d_unigrams, d_bigrams, d_trigrams]\n",
    "\n",
    "    unknown_text = batch.iloc[-1].text\n",
    "    unknown_words = preprocess(unknown_text)\n",
    "    u_word_ngrams = make_word_ngrams(unknown_words)\n",
    "    if n_char is not None:\n",
    "        u_char_ngrams = make_char_ngrams(unknown_text, n_char)\n",
    "\n",
    "    if n_char is not None:\n",
    "        answer, mean_r = classify(d_char_ngrams, u_char_ngrams, theta)\n",
    "    else:\n",
    "        answer, mean_r = classify(\n",
    "            d_word_ngrams[n_word - 1], u_word_ngrams[n_word - 1], theta)\n",
    "    return answer, mean_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функции для обучения и предсказания\n",
    "\\+ make_batches для выделения групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(lang_df):  # создает список папок на одном языке\n",
    "    batches = []\n",
    "\n",
    "    authors = sorted(list(set(lang_df['author'].values)))\n",
    "    for author in authors:\n",
    "        batch_df = lang_df[lang_df['author'] == author]\n",
    "        batches.append(batch_df)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, answers, n_chars=None, n_words=1):  # подбор теты\n",
    "    batches = make_batches(df)\n",
    "    X = []\n",
    "    for batch in batches:\n",
    "        answer, mean_r = verify_authorship(batch, 1, n_chars, n_words)\n",
    "        X.append(mean_r)\n",
    "    X = np.array(X)\n",
    "    X = X[:, np.newaxis]\n",
    "\n",
    "    y = np.array(answers)\n",
    "    y[y == 'Y'] = 1\n",
    "    y[y == 'N'] = 0\n",
    "    y = y.astype(int)\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    theta = (clf.intercept_/-clf.coef_)[0][0]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, theta, n_chars=None, n_words=1):  # выдает ответы\n",
    "    batches = make_batches(df)\n",
    "    predictions = []\n",
    "    for batch in batches:\n",
    "        answer, mean_r = verify_authorship(batch, theta, n_chars, n_words)\n",
    "        predictions.append(answer)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перебор типов n-грамм и подсчет accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts('./texts_train/')\n",
    "df = make_df(texts)\n",
    "\n",
    "test_texts = get_texts('./texts_test/')\n",
    "test_df = make_df(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(truth_file):  # получает ответы из файла\n",
    "    with open(truth_file, 'r', encoding='utf-8-sig') as f:\n",
    "        truth_text = f.read()\n",
    "\n",
    "    answers = []\n",
    "    answers_temp = truth_text.split('\\n')\n",
    "    for ans in answers_temp:\n",
    "        if ans != '':\n",
    "            answers.append(ans.split()[1])\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(lang_df, test_lang_df, answers, answers_test, eval_type):\n",
    "    # eval_type = 'char' or 'word'\n",
    "    if eval_type == 'char':\n",
    "        r_start = 3\n",
    "        r_finish = 11\n",
    "        arg = 'n_chars'\n",
    "    if eval_type == 'word':\n",
    "        r_start = 1\n",
    "        r_finish = 4\n",
    "        arg = 'n_words'\n",
    "    preds = []\n",
    "    for i in range(r_start, r_finish):\n",
    "        print(f'Train on {eval_type} {i}-grams...')\n",
    "        kwarg = {arg : i}\n",
    "        theta = train(lang_df, answers, **kwarg)\n",
    "        predictions = predict(lang_df, theta, **kwarg)\n",
    "        accuracy = accuracy_score(answers, predictions)\n",
    "        print(f'Accuracy on train:\\t{(accuracy*100):.2f}%')\n",
    "        print(f'Theta:\\t\\t\\t{theta:.3f}')\n",
    "\n",
    "        predictions = predict(test_lang_df, theta, **kwarg)\n",
    "        accuracy = accuracy_score(answers_test, predictions)\n",
    "        print(f'Accuracy on test:\\t{(accuracy*100):.2f}%')\n",
    "        preds.append(predictions) # это тестовые\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'gr', 'sp']\n"
     ]
    }
   ],
   "source": [
    "lang_list = sorted(list(set(df['lang'].values)))\n",
    "print(lang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = get_answers('./answers/truth.txt')\n",
    "answers_test = get_answers('./answers/truth-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_testing(lang, lang_short, slicing, df, test_df, eval_type):\n",
    "    print(f'Evaluating on {lang} data...\\n')\n",
    "    lang_df = df[df['lang'] == lang_short]\n",
    "    lang_test_df = test_df[test_df['lang'] == lang_short]\n",
    "    preds = evaluate(lang_df,\n",
    "                     lang_test_df,\n",
    "                     answers[slicing[0]],\n",
    "                     answers_test[slicing[1]],\n",
    "                     eval_type)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(lang, all_preds, answers_test):\n",
    "    # тестирует ансабль классификаторов\n",
    "    print(f'Calculating ensembles on {lang} data...\\n')\n",
    "    preds = np.array(all_preds).T\n",
    "    best_preds = [Counter(pred).most_common(1)[0][0] for pred in preds]\n",
    "    accuracy = accuracy_score(answers_test, best_preds)\n",
    "    print(f'Accuracy on test:\\t{(accuracy*100):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on English data...\n",
      "\n",
      "Train on char 3-grams...\n",
      "Accuracy on train:\t80.00%\n",
      "Theta:\t\t\t1.103\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 4-grams...\n",
      "Accuracy on train:\t80.00%\n",
      "Theta:\t\t\t1.077\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 5-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.068\n",
      "Accuracy on test:\t63.33%\n",
      "Train on char 6-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.063\n",
      "Accuracy on test:\t63.33%\n",
      "Train on char 7-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.058\n",
      "Accuracy on test:\t63.33%\n",
      "Train on char 8-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.054\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 9-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.048\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 10-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.044\n",
      "Accuracy on test:\t60.00%\n",
      "Evaluating on English data...\n",
      "\n",
      "Train on word 1-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.105\n",
      "Accuracy on test:\t60.00%\n",
      "Train on word 2-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.045\n",
      "Accuracy on test:\t56.67%\n",
      "Train on word 3-grams...\n",
      "Accuracy on train:\t40.00%\n",
      "Theta:\t\t\t1.029\n",
      "Accuracy on test:\t63.33%\n",
      "Calculating ensembles on English char data...\n",
      "\n",
      "Accuracy on test:\t63.33%\n",
      "Calculating ensembles on English word data...\n",
      "\n",
      "Accuracy on test:\t56.67%\n"
     ]
    }
   ],
   "source": [
    "# English\n",
    "all_preds_char = lang_testing(\n",
    "    'English', 'en', [slice(None, 10), slice(None, 30)], df, test_df, 'char')\n",
    "all_preds_word = lang_testing(\n",
    "    'English', 'en', [slice(None, 10), slice(None, 30)], df, test_df, 'word')\n",
    "ensemble('English char', all_preds_char, answers_test[:30])\n",
    "ensemble('English word', all_preds_word, answers_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Greek data...\n",
      "\n",
      "Train on char 3-grams...\n",
      "Accuracy on train:\t75.00%\n",
      "Theta:\t\t\t1.144\n",
      "Accuracy on test:\t63.33%\n",
      "Train on char 4-grams...\n",
      "Accuracy on train:\t85.00%\n",
      "Theta:\t\t\t1.185\n",
      "Accuracy on test:\t66.67%\n",
      "Train on char 5-grams...\n",
      "Accuracy on train:\t85.00%\n",
      "Theta:\t\t\t1.202\n",
      "Accuracy on test:\t63.33%\n",
      "Train on char 6-grams...\n",
      "Accuracy on train:\t80.00%\n",
      "Theta:\t\t\t1.210\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 7-grams...\n",
      "Accuracy on train:\t80.00%\n",
      "Theta:\t\t\t1.214\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 8-grams...\n",
      "Accuracy on train:\t80.00%\n",
      "Theta:\t\t\t1.216\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 9-grams...\n",
      "Accuracy on train:\t75.00%\n",
      "Theta:\t\t\t1.216\n",
      "Accuracy on test:\t60.00%\n",
      "Train on char 10-grams...\n",
      "Accuracy on train:\t75.00%\n",
      "Theta:\t\t\t1.215\n",
      "Accuracy on test:\t56.67%\n",
      "Evaluating on Greek data...\n",
      "\n",
      "Train on word 1-grams...\n",
      "Accuracy on train:\t85.00%\n",
      "Theta:\t\t\t1.224\n",
      "Accuracy on test:\t63.33%\n",
      "Train on word 2-grams...\n",
      "Accuracy on train:\t75.00%\n",
      "Theta:\t\t\t1.217\n",
      "Accuracy on test:\t53.33%\n",
      "Train on word 3-grams...\n",
      "Accuracy on train:\t70.00%\n",
      "Theta:\t\t\t1.213\n",
      "Accuracy on test:\t53.33%\n",
      "Calculating ensembles on Greek char data...\n",
      "\n",
      "Accuracy on test:\t60.00%\n",
      "Calculating ensembles on Greek word data...\n",
      "\n",
      "Accuracy on test:\t53.33%\n"
     ]
    }
   ],
   "source": [
    "# Greek\n",
    "all_preds_char = lang_testing(\n",
    "    'Greek', 'gr', [slice(10, 30), slice(30, 60)], df, test_df, 'char')\n",
    "all_preds_word = lang_testing(\n",
    "    'Greek', 'gr', [slice(10, 30), slice(30, 60)], df, test_df, 'word')\n",
    "ensemble('Greek char', all_preds_char, answers_test[30:60])\n",
    "ensemble('Greek word', all_preds_word, answers_test[30:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Spanish data...\n",
      "\n",
      "Train on char 3-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.205\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 4-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.260\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 5-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.322\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 6-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.362\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 7-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.362\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 8-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.375\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 9-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.396\n",
      "Accuracy on test:\t52.00%\n",
      "Train on char 10-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.408\n",
      "Accuracy on test:\t52.00%\n",
      "Evaluating on Spanish data...\n",
      "\n",
      "Train on word 1-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.380\n",
      "Accuracy on test:\t52.00%\n",
      "Train on word 2-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.543\n",
      "Accuracy on test:\t52.00%\n",
      "Train on word 3-grams...\n",
      "Accuracy on train:\t60.00%\n",
      "Theta:\t\t\t2.548\n",
      "Accuracy on test:\t52.00%\n",
      "Calculating ensembles on Spanish char data...\n",
      "\n",
      "Accuracy on test:\t52.00%\n",
      "Calculating ensembles on Spanish word data...\n",
      "\n",
      "Accuracy on test:\t52.00%\n"
     ]
    }
   ],
   "source": [
    "# Spanish\n",
    "all_preds_char = lang_testing(\n",
    "    'Spanish', 'sp', [slice(30, None), slice(60, None)], df, test_df, 'char')\n",
    "all_preds_word = lang_testing(\n",
    "    'Spanish', 'sp', [slice(30, None), slice(60, None)], df, test_df, 'word')\n",
    "ensemble('Spanish char', all_preds_char, answers_test[60:])\n",
    "ensemble('Spanish word', all_preds_word, answers_test[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
